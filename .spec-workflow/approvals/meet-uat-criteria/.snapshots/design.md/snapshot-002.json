{
  "id": "snapshot_1764714554416_kxglcdwd2",
  "approvalId": "approval_1764714504044_zdsqfbn73",
  "approvalTitle": "Meet UAT Criteria - Design Document",
  "version": 2,
  "timestamp": "2025-12-02T22:29:14.416Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Design Document\n\n## Overview\n\nThis design implements an **Automated UAT System** that shifts traditional User Acceptance Testing left into the development and CI/CD pipeline. By leveraging KeyRx's unique capabilities—deterministic input/output, session recording/replay, CLI-first architecture—we can automate the vast majority of UAT scenarios, reducing manual testing to smoke verification only.\n\nThe system extends the existing Rhai test harness with UAT-specific primitives, introduces golden session management for regression detection, and provides configurable quality gates for release readiness.\n\n## Steering Document Alignment\n\n### Technical Standards (tech.md)\n\n- **CLI First**: All UAT commands available via `keyrx uat`, `keyrx regression`, `keyrx ci-check`\n- **Rhai Scripting**: Uses existing test harness infrastructure (`simulate_tap()`, `assert_output()`)\n- **Session Recording**: Extends existing `.krx` session format for golden sessions\n- **Performance Targets**: Enforces <1ms latency requirement, CI fails on >100µs regression\n- **Fuzz Testing**: Extends existing proptest/fuzz infrastructure\n\n### Project Structure (structure.md)\n\n```\nkeyrx/\n├── core/\n│   └── src/\n│       ├── uat/                    # NEW: UAT system module\n│       │   ├── mod.rs              # Module exports\n│       │   ├── runner.rs           # UAT test discovery & execution\n│       │   ├── golden.rs           # Golden session management\n│       │   ├── gates.rs            # Quality gate enforcement\n│       │   ├── coverage.rs         # Requirements coverage mapping\n│       │   ├── perf.rs             # Performance UAT\n│       │   ├── fuzz.rs             # Fuzz-based chaos testing\n│       │   └── report.rs           # Report generation\n│       └── cli/\n│           └── commands/\n│               ├── uat.rs          # NEW: keyrx uat command\n│               ├── regression.rs   # NEW: keyrx regression command\n│               ├── golden.rs       # NEW: keyrx record-golden, verify-golden\n│               └── ci_check.rs     # NEW: keyrx ci-check command\n├── tests/\n│   ├── uat/                        # NEW: UAT test scripts\n│   │   ├── core/                   # Core functionality UAT\n│   │   ├── layers/                 # Layer switching UAT\n│   │   └── combos/                 # Combo execution UAT\n│   └── golden/                     # NEW: Golden session recordings\n│       ├── basic_typing.krx\n│       ├── layer_switch.krx\n│       └── combo_execution.krx\n├── .keyrx/\n│   └── quality-gates.toml          # NEW: Quality gate definitions\n└── target/\n    └── uat-report/                 # Generated reports\n```\n\n## Code Reuse Analysis\n\n### Existing Components to Leverage\n\n- **`core/src/test_harness/`**: Existing Rhai test infrastructure\n  - `simulate_tap()`, `simulate_hold()`, `assert_output()` functions\n  - Script discovery and execution engine\n  - Will extend with `uat_*` prefix handling and metadata parsing\n\n- **`core/src/session/`**: Session recording/replay\n  - `.krx` format for event serialization\n  - Replay engine for deterministic reproduction\n  - Will extend with semantic comparison (ignoring non-deterministic fields)\n\n- **`core/src/cli/`**: Existing CLI framework\n  - Subcommand structure and argument parsing\n  - Will add new subcommands: `uat`, `regression`, `record-golden`, etc.\n\n- **`core/benches/latency.rs`**: Existing benchmark infrastructure\n  - Criterion-based latency measurement\n  - Will integrate with performance UAT for threshold enforcement\n\n### Integration Points\n\n- **Test Runner**: UAT runner wraps existing test harness\n- **Session Format**: Golden sessions use existing `.krx` format with metadata extension\n- **CI Pipeline**: `ci-check` command integrates with existing CI workflow\n- **Metrics Collection**: Uses existing timing infrastructure for latency capture\n\n## Architecture\n\n### System Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                            UAT Command Layer                                 │\n│  ┌─────────────┐  ┌──────────────┐  ┌────────────┐  ┌─────────────────────┐ │\n│  │ keyrx uat   │  │keyrx         │  │keyrx       │  │ keyrx ci-check     │ │\n│  │             │  │regression    │  │*-golden    │  │                     │ │\n│  └──────┬──────┘  └──────┬───────┘  └─────┬──────┘  └──────────┬──────────┘ │\n└─────────┼────────────────┼────────────────┼────────────────────┼────────────┘\n          │                │                │                    │\n          ▼                ▼                ▼                    ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                           UAT Core Engine                                    │\n│                                                                              │\n│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────────────────┐  │\n│  │  Test Runner    │  │ Golden Session  │  │     Quality Gate            │  │\n│  │  - Discovery    │  │   Manager       │  │       Enforcer              │  │\n│  │  - Execution    │  │  - Record       │  │  - Config loading           │  │\n│  │  - Metrics      │  │  - Replay       │  │  - Threshold evaluation     │  │\n│  │  - Categories   │  │  - Compare      │  │  - Gate selection           │  │\n│  └────────┬────────┘  └────────┬────────┘  └─────────────┬───────────────┘  │\n│           │                    │                          │                  │\n│           ▼                    ▼                          ▼                  │\n│  ┌─────────────────────────────────────────────────────────────────────────┐│\n│  │                    Shared Infrastructure                                 ││\n│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────────┐ ││\n│  │  │ Coverage    │  │ Performance │  │ Fuzz        │  │ Report          │ ││\n│  │  │ Mapper      │  │ UAT         │  │ Engine      │  │ Generator       │ ││\n│  │  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────────┘ ││\n│  └─────────────────────────────────────────────────────────────────────────┘│\n└─────────────────────────────────────────────────────────────────────────────┘\n          │                    │                          │\n          ▼                    ▼                          ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         Existing Infrastructure                              │\n│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────────────────┐  │\n│  │ Rhai Test       │  │ Session         │  │ Benchmark                   │  │\n│  │ Harness         │  │ Recording       │  │ Infrastructure              │  │\n│  │ (test_harness/) │  │ (session/)      │  │ (benches/)                  │  │\n│  └─────────────────┘  └─────────────────┘  └─────────────────────────────┘  │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n### Data Flow\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                         UAT Execution Flow                                │\n│                                                                           │\n│  1. Discovery          2. Execution          3. Evaluation                │\n│  ┌─────────────┐       ┌─────────────┐       ┌─────────────┐             │\n│  │ Scan tests/ │──────▶│ Run tests   │──────▶│ Apply       │             │\n│  │ for uat_*   │       │ with timing │       │ quality     │             │\n│  │ functions   │       │ metrics     │       │ gates       │             │\n│  └─────────────┘       └─────────────┘       └──────┬──────┘             │\n│        │                     │                      │                     │\n│        ▼                     ▼                      ▼                     │\n│  ┌─────────────┐       ┌─────────────┐       ┌─────────────┐             │\n│  │ Parse       │       │ Collect:    │       │ Determine   │             │\n│  │ metadata:   │       │ - Results   │       │ pass/fail   │             │\n│  │ @category   │       │ - Timing    │       │ per gate    │             │\n│  │ @priority   │       │ - Coverage  │       │ criteria    │             │\n│  │ @requirement│       │             │       │             │             │\n│  └─────────────┘       └─────────────┘       └──────┬──────┘             │\n│                                                      │                    │\n│  4. Reporting                                        │                    │\n│  ┌────────────────────────────────────────────┐      │                    │\n│  │ Generate report with:                       │◀────┘                    │\n│  │ - Summary (pass/fail counts)               │                           │\n│  │ - Test results by category                 │                           │\n│  │ - Coverage matrix                          │                           │\n│  │ - Performance metrics                      │                           │\n│  │ - Quality gate status                      │                           │\n│  └────────────────────────────────────────────┘                           │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### Modular Design Principles\n\n- **Single Responsibility**: Each module handles one concern (runner, golden, gates, etc.)\n- **Component Isolation**: UAT system is a separate module from existing test harness\n- **Extensibility**: New UAT primitives can be added without modifying core\n- **Fail Fast**: Quality gates evaluated immediately after test execution\n\n## Components and Interfaces\n\n### Component 1: UAT Test Runner (`uat/runner.rs`)\n\n**Purpose:** Discover, categorize, and execute UAT tests with metrics collection\n\n**Interfaces:**\n```rust\npub struct UatRunner {\n    test_dir: PathBuf,\n    harness: TestHarness,\n}\n\nimpl UatRunner {\n    /// Discover all uat_* functions in test files\n    pub fn discover(&self) -> Result<Vec<UatTest>>;\n\n    /// Run tests matching filter criteria\n    pub fn run(&self, filter: UatFilter) -> Result<UatResults>;\n\n    /// Run with fail-fast behavior\n    pub fn run_fail_fast(&self, filter: UatFilter) -> Result<UatResults>;\n}\n\npub struct UatFilter {\n    pub categories: Option<Vec<String>>,\n    pub priorities: Option<Vec<Priority>>,\n    pub pattern: Option<String>,\n}\n\npub struct UatTest {\n    pub name: String,\n    pub file: PathBuf,\n    pub category: Option<String>,\n    pub priority: Priority,\n    pub requirements: Vec<String>,\n    pub latency_threshold: Option<Duration>,\n}\n\npub struct UatResults {\n    pub total: usize,\n    pub passed: usize,\n    pub failed: usize,\n    pub skipped: usize,\n    pub duration: Duration,\n    pub tests: Vec<UatTestResult>,\n}\n```\n\n**Dependencies:** TestHarness, Rhai runtime\n**Reuses:** `core/src/test_harness/` discovery and execution logic\n\n### Component 2: Golden Session Manager (`uat/golden.rs`)\n\n**Purpose:** Record, store, and compare golden session recordings\n\n**Interfaces:**\n```rust\npub struct GoldenSessionManager {\n    golden_dir: PathBuf,\n}\n\nimpl GoldenSessionManager {\n    /// Record a new golden session from script execution\n    pub fn record(&self, name: &str, script: &Path) -> Result<GoldenSession>;\n\n    /// Verify current output matches golden session\n    pub fn verify(&self, name: &str) -> Result<GoldenVerifyResult>;\n\n    /// Update existing golden session\n    pub fn update(&self, name: &str, confirm: bool) -> Result<()>;\n\n    /// List all golden sessions\n    pub fn list(&self) -> Result<Vec<GoldenSessionInfo>>;\n}\n\npub struct GoldenSession {\n    pub name: String,\n    pub created: DateTime<Utc>,\n    pub events: Vec<RecordedEvent>,\n    pub outputs: Vec<ExpectedOutput>,\n}\n\npub struct GoldenVerifyResult {\n    pub matches: bool,\n    pub differences: Vec<Difference>,\n}\n\npub struct Difference {\n    pub event_index: usize,\n    pub expected: String,\n    pub actual: String,\n    pub diff_type: DiffType,\n}\n```\n\n**Dependencies:** Session recording module\n**Reuses:** `core/src/session/` format and replay logic\n\n### Component 3: Quality Gate Enforcer (`uat/gates.rs`)\n\n**Purpose:** Load and enforce quality gate configurations\n\n**Interfaces:**\n```rust\npub struct QualityGateEnforcer {\n    config_path: PathBuf,\n}\n\nimpl QualityGateEnforcer {\n    /// Load quality gate configuration\n    pub fn load(&self, gate_name: Option<&str>) -> Result<QualityGate>;\n\n    /// Evaluate results against quality gate\n    pub fn evaluate(&self, gate: &QualityGate, results: &UatResults) -> GateResult;\n}\n\npub struct QualityGate {\n    pub name: String,\n    pub pass_rate: Option<f64>,         // e.g., 95.0\n    pub p0_open: Option<u32>,            // e.g., 0\n    pub p1_open: Option<u32>,            // e.g., 2\n    pub max_latency_us: Option<u64>,     // e.g., 1000\n    pub coverage_min: Option<f64>,       // e.g., 80.0\n}\n\npub struct GateResult {\n    pub passed: bool,\n    pub violations: Vec<GateViolation>,\n}\n\npub struct GateViolation {\n    pub criterion: String,\n    pub expected: String,\n    pub actual: String,\n}\n```\n\n**Dependencies:** None\n**Reuses:** TOML parsing patterns from existing config loading\n\n### Component 4: Coverage Mapper (`uat/coverage.rs`)\n\n**Purpose:** Track requirement-to-test traceability\n\n**Interfaces:**\n```rust\npub struct CoverageMapper {\n    requirements_path: PathBuf,\n}\n\nimpl CoverageMapper {\n    /// Build coverage map from test metadata\n    pub fn build(&self, tests: &[UatTest], results: &UatResults) -> CoverageMap;\n\n    /// Generate coverage report\n    pub fn report(&self, map: &CoverageMap) -> CoverageReport;\n}\n\npub struct CoverageMap {\n    pub requirements: HashMap<String, RequirementCoverage>,\n}\n\npub struct RequirementCoverage {\n    pub id: String,\n    pub linked_tests: Vec<String>,\n    pub status: CoverageStatus,\n    pub last_verified: Option<DateTime<Utc>>,\n}\n\npub enum CoverageStatus {\n    Verified,    // All linked tests pass\n    AtRisk,      // Some linked tests fail\n    Uncovered,   // No linked tests\n}\n```\n\n**Dependencies:** UatRunner\n**Reuses:** Requirement parsing from spec-workflow\n\n### Component 5: Performance UAT (`uat/perf.rs`)\n\n**Purpose:** Verify latency requirements are met\n\n**Interfaces:**\n```rust\npub struct PerformanceUat {\n    baseline_path: Option<PathBuf>,\n}\n\nimpl PerformanceUat {\n    /// Run performance UAT tests\n    pub fn run(&self, filter: UatFilter) -> Result<PerfResults>;\n\n    /// Compare against baseline\n    pub fn compare_baseline(&self, branch: &str) -> Result<PerfComparison>;\n}\n\npub struct PerfResults {\n    pub p50: Duration,\n    pub p95: Duration,\n    pub p99: Duration,\n    pub max: Duration,\n    pub violations: Vec<LatencyViolation>,\n}\n\npub struct LatencyViolation {\n    pub test_name: String,\n    pub event_index: usize,\n    pub threshold_us: u64,\n    pub actual_us: u64,\n}\n```\n\n**Dependencies:** Benchmark infrastructure\n**Reuses:** `core/benches/latency.rs` timing measurement\n\n### Component 6: Fuzz Engine (`uat/fuzz.rs`)\n\n**Purpose:** Generate random inputs for chaos testing\n\n**Interfaces:**\n```rust\npub struct FuzzEngine {\n    crash_dir: PathBuf,\n}\n\nimpl FuzzEngine {\n    /// Run fuzz testing for specified duration\n    pub fn run(&self, duration: Duration, count: Option<u64>) -> Result<FuzzResults>;\n\n    /// Replay a crash sequence\n    pub fn replay_crash(&self, crash_file: &Path) -> Result<()>;\n}\n\npub struct FuzzResults {\n    pub sequences_tested: u64,\n    pub duration: Duration,\n    pub unique_paths: u64,\n    pub crashes: Vec<CrashInfo>,\n}\n\npub struct CrashInfo {\n    pub sequence_file: PathBuf,\n    pub error: String,\n    pub timestamp: DateTime<Utc>,\n}\n```\n\n**Dependencies:** KeyEvent generation, Engine\n**Reuses:** Existing fuzz testing infrastructure\n\n### Component 7: Report Generator (`uat/report.rs`)\n\n**Purpose:** Generate comprehensive UAT reports\n\n**Interfaces:**\n```rust\npub struct ReportGenerator {\n    output_dir: PathBuf,\n}\n\nimpl ReportGenerator {\n    /// Generate HTML report\n    pub fn generate_html(&self, data: &ReportData) -> Result<PathBuf>;\n\n    /// Generate Markdown report (for PR comments)\n    pub fn generate_markdown(&self, data: &ReportData) -> Result<String>;\n\n    /// Generate JSON report (machine-readable)\n    pub fn generate_json(&self, data: &ReportData) -> Result<String>;\n}\n\npub struct ReportData {\n    pub summary: UatSummary,\n    pub test_results: Vec<UatTestResult>,\n    pub coverage: CoverageMap,\n    pub performance: PerfResults,\n    pub gate_status: GateResult,\n    pub trend: Option<TrendData>,\n}\n```\n\n**Dependencies:** All UAT components\n**Reuses:** Template patterns from existing report generation\n\n### Component 8: CI Check Command (`cli/commands/ci_check.rs`)\n\n**Purpose:** Unified CI command that runs all checks\n\n**Interfaces:**\n```rust\npub struct CiCheckCommand {\n    gate: Option<String>,\n    json_output: bool,\n}\n\nimpl CiCheckCommand {\n    /// Run all CI checks\n    pub fn run(&self) -> Result<CiCheckResults>;\n}\n\npub struct CiCheckResults {\n    pub unit_tests: TestSuiteResult,\n    pub integration_tests: TestSuiteResult,\n    pub uat_tests: UatResults,\n    pub regression_tests: RegressionResults,\n    pub performance_tests: PerfResults,\n    pub gate_result: GateResult,\n}\n\n/// Exit codes\npub const EXIT_SUCCESS: i32 = 0;\npub const EXIT_ERROR: i32 = 1;\npub const EXIT_TEST_FAIL: i32 = 2;\npub const EXIT_GATE_FAIL: i32 = 3;\n```\n\n**Dependencies:** All test runners, quality gates\n**Reuses:** Existing CLI framework\n\n## Data Models\n\n### Quality Gate Configuration (TOML)\n\n```toml\n# .keyrx/quality-gates.toml\n\n[default]\npass_rate = 95.0\np0_open = 0\np1_open = 2\nmax_latency_us = 1000\ncoverage_min = 80.0\n\n[alpha]\npass_rate = 80.0\np0_open = 0\np1_open = 5\ncoverage_min = 60.0\n\n[beta]\npass_rate = 90.0\np0_open = 0\np1_open = 2\ncoverage_min = 75.0\n\n[rc]\npass_rate = 98.0\np0_open = 0\np1_open = 0\nmax_latency_us = 500\ncoverage_min = 85.0\n\n[ga]\npass_rate = 100.0\np0_open = 0\np1_open = 0\nmax_latency_us = 500\ncoverage_min = 90.0\n```\n\n### UAT Test Metadata (Rhai Comments)\n\n```rhai\n// @category: core\n// @priority: P0\n// @requirement: 1.1, 1.2\n// @latency: 1000\n\nfn uat_basic_key_mapping() {\n    simulate_tap(KEY_A);\n    assert_output(\"a\");\n    assert_timing(0, 1000);  // 0-1000µs\n}\n```\n\n### Golden Session Format (JSON)\n\n```json\n{\n  \"name\": \"basic_typing\",\n  \"version\": \"1.0\",\n  \"created\": \"2024-12-03T10:00:00Z\",\n  \"metadata\": {\n    \"description\": \"Basic key-to-character mapping\",\n    \"requirements\": [\"1.1\"]\n  },\n  \"events\": [\n    {\"type\": \"key_press\", \"code\": 30, \"time_us\": 0},\n    {\"type\": \"key_release\", \"code\": 30, \"time_us\": 50000}\n  ],\n  \"expected_outputs\": [\n    {\"index\": 0, \"output\": \"a\", \"timing_range_us\": [0, 1000]}\n  ]\n}\n```\n\n### UAT Results (JSON)\n\n```json\n{\n  \"total\": 100,\n  \"passed\": 95,\n  \"failed\": 3,\n  \"skipped\": 2,\n  \"duration_ms\": 5000,\n  \"by_category\": {\n    \"core\": {\"total\": 50, \"passed\": 49, \"failed\": 1},\n    \"layers\": {\"total\": 30, \"passed\": 28, \"failed\": 2},\n    \"combos\": {\"total\": 20, \"passed\": 18, \"failed\": 0, \"skipped\": 2}\n  },\n  \"by_priority\": {\n    \"P0\": {\"total\": 20, \"passed\": 20, \"failed\": 0},\n    \"P1\": {\"total\": 40, \"passed\": 38, \"failed\": 2},\n    \"P2\": {\"total\": 40, \"passed\": 37, \"failed\": 1, \"skipped\": 2}\n  },\n  \"failed_tests\": [\n    {\"name\": \"uat_layer_rapid_switch\", \"error\": \"Assertion failed: expected layer 2, got layer 1\"}\n  ]\n}\n```\n\n## Error Handling\n\n### Error Scenarios\n\n1. **UAT test discovery fails (no tests found)**\n   - **Handling:** Return warning, exit with success (not an error)\n   - **User Impact:** Message: \"No UAT tests found in tests/uat/\"\n\n2. **Golden session not found**\n   - **Handling:** Return error with helpful message\n   - **User Impact:** \"Golden session 'foo' not found. Run 'keyrx record-golden foo' first.\"\n\n3. **Quality gate config not found**\n   - **Handling:** Use default quality gate\n   - **User Impact:** Warning: \"Using default quality gate. Create .keyrx/quality-gates.toml to customize.\"\n\n4. **Latency threshold exceeded**\n   - **Handling:** Mark test as failed, include in report\n   - **User Impact:** Report shows exact event and latency: \"Event 5 took 1500µs (threshold: 1000µs)\"\n\n5. **Fuzz testing crash**\n   - **Handling:** Save sequence to tests/crashes/, continue testing\n   - **User Impact:** \"Crash detected! Saved to tests/crashes/2024-12-03T10-00-00.krx\"\n\n6. **Report generation fails**\n   - **Handling:** Fall back to console output\n   - **User Impact:** Warning: \"Could not write report to file. Displaying in console.\"\n\n### Exit Codes\n\n| Code | Meaning | When |\n|------|---------|------|\n| 0 | Success | All tests pass, all gates pass |\n| 1 | Error | System error (config parse, file not found) |\n| 2 | Test Failure | One or more tests failed |\n| 3 | Gate Failure | Tests passed but gate criteria not met |\n\n## Testing Strategy\n\n### Unit Testing\n\n- **Runner tests:** Test metadata parsing, filter matching, result aggregation\n- **Golden tests:** Test comparison logic, diff generation\n- **Gate tests:** Test threshold evaluation, violation detection\n- **Coverage tests:** Test requirement linking, status calculation\n\n### Integration Testing\n\n- **Full UAT flow:** Discovery → Execution → Gate evaluation → Report\n- **Golden session lifecycle:** Record → Verify → Update\n- **CI check flow:** All test types → Consolidated report\n\n### End-to-End Testing\n\n- **New developer scenario:** Clone → Setup → Run UAT → All pass\n- **Regression scenario:** Make breaking change → Regression detected → Fix → Pass\n- **Release scenario:** Run ci-check with RC gate → Generate release report\n\n## Implementation Sequence\n\n1. **Core UAT runner** (runner.rs) - Foundation for test discovery and execution\n2. **Metadata parsing** - @category, @priority, @requirement parsing\n3. **Quality gates** (gates.rs) - Gate configuration and evaluation\n4. **CLI commands** - keyrx uat command\n5. **Golden sessions** (golden.rs) - Record/verify/update functionality\n6. **Regression command** - keyrx regression command\n7. **Coverage mapping** (coverage.rs) - Requirements traceability\n8. **Performance UAT** (perf.rs) - Latency verification\n9. **Fuzz engine** (fuzz.rs) - Chaos testing\n10. **Report generator** (report.rs) - HTML/Markdown/JSON output\n11. **CI check command** - Unified CI command\n12. **Integration tests** - End-to-end verification\n",
  "fileStats": {
    "size": 26234,
    "lines": 666,
    "lastModified": "2025-12-02T22:28:18.855Z"
  },
  "comments": []
}