{
  "id": "snapshot_1765468849503_9mbnwv2l2",
  "approvalId": "approval_1765468849438_oonwq1b73",
  "approvalTitle": "Fix Failing Tests - Tasks Document (16 tasks)",
  "version": 1,
  "timestamp": "2025-12-11T16:00:49.503Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Tasks Document\n\n## Phase 1: Investigation and Root Cause Analysis\n\n- [ ] 1.1 Reproduce test failures locally\n  - Command: `cargo test test_c_api_null_label_clears test_macro_generates_doc -- --exact`\n  - Verify both tests fail with expected errors\n  - Capture exact error messages\n  - Purpose: Confirm failures and understand errors\n  - _Leverage: cargo test_\n  - _Requirements: 1.1, 2.1_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: QA engineer\n\nTask: Reproduce test failures following requirements 1.1 and 2.1. Run `cargo test test_c_api_null_label_clears -- --exact` and `cargo test test_macro_generates_doc -- --exact`. Capture full output including error messages, stack traces, and any debug output. Save to .spec-workflow/specs/fix-failing-tests/failure_reproduction.txt. Verify errors match: (1) \"assertion failed: msg.starts_with(\"ok:\")\" for FFI test, (2) \"Documentation should be registered\" for doc test.\n\nRestrictions: Just reproduce and document. Do not attempt fixes yet. Ensure failures are consistent across multiple runs.\n\nSuccess: Both test failures reproduced and documented. Exact error messages captured. Failures are consistent and match expectations. Ready for root cause analysis.\n\nAfter completing:\n1. Mark [-] before starting\n2. Run tests and capture output\n3. Use log-implementation to record findings\n4. Mark [x] when complete_\n\n- [ ] 1.2 Analyze FFI device registry test failure\n  - File: core/src/ffi/domains/device_registry.rs:571\n  - Read test code and understand what it's testing\n  - Read implementation of set_device_label FFI function\n  - Check actual response format returned\n  - Purpose: Identify root cause of FFI test failure\n  - _Leverage: Test code, implementation code_\n  - _Requirements: 1.1_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Rust debugging specialist\n\nTask: Analyze FFI test failure following requirement 1.1. Read `test_c_api_null_label_clears` test in core/src/ffi/domains/device_registry.rs around line 571. Understand: (1) What test is validating, (2) What response format it expects, (3) What device_key and null label should do. Then read FFI implementation function that's called. Add temporary debug println! to see actual response. Run test again to capture actual vs expected. Document findings in .spec-workflow/specs/fix-failing-tests/ffi_test_analysis.md with root cause conclusion.\n\nRestrictions: Analysis only. Add debug output temporarily but don't fix yet. Identify one of: format changed, null handling wrong, error returned incorrectly, test expectation wrong.\n\nSuccess: Root cause identified and documented. Understand why test fails. Know what needs to change (test expectation vs implementation). Clear fix strategy documented.\n\nAfter completing:\n1. Mark [-], analyze, document findings, log analysis, mark [x]_\n\n- [ ] 1.3 Analyze scripting documentation test failure\n  - File: core/src/scripting/docs/test_example.rs:46\n  - Read test code and understand what it's testing\n  - Check doc registry initialization\n  - Review similar passing doc tests for patterns\n  - Purpose: Identify root cause of doc test failure\n  - _Leverage: Test code, doc registry code_\n  - _Requirements: 2.1_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Rust debugging specialist\n\nTask: Analyze doc test failure following requirement 2.1. Read `test_macro_generates_doc` test in core/src/scripting/docs/test_example.rs around line 46. Understand: (1) What `#[rhai_doc]` macro should do, (2) What registry should contain, (3) What test is checking. Find other passing doc tests and compare setup. Check if initialize() is called. Check if macro is working. Add debug output to see registry state. Document findings in .spec-workflow/specs/fix-failing-tests/doc_test_analysis.md with root cause conclusion.\n\nRestrictions: Analysis only. Identify one of: registry not initialized, macro not registering, wrong function name, test isolation issue.\n\nSuccess: Root cause identified. Understand why test fails. Know what needs to change. Clear fix strategy documented.\n\nAfter completing:\n1. Mark [-], analyze, document, log, mark [x]_\n\n## Phase 2: Implement Fixes\n\n- [ ] 2.1 Fix FFI device registry test\n  - File: core/src/ffi/domains/device_registry.rs (test or implementation)\n  - Apply fix based on root cause from 1.2\n  - Update test expectation or fix implementation as needed\n  - Remove temporary debug code\n  - Purpose: Make test pass\n  - _Leverage: Root cause analysis from 1.2_\n  - _Requirements: 1.1, 1.2, 1.3, 1.4, 1.5_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Rust developer with FFI expertise\n\nTask: Fix FFI test following requirements 1.1-1.5. Based on root cause from 1.2, apply appropriate fix: (A) If format changed - update test assertion to expect new format, (B) If null handling wrong - fix implementation to treat null as clear, (C) If error returned - fix error handling to return success on null, (D) If test wrong - fix test logic. Make minimal changes. Add comment explaining fix. Remove debug code.\n\nRestrictions: Minimal fix only. Don't refactor unrelated code. Preserve test intent. Ensure null label clears device label correctly. Don't break other device_registry tests.\n\nSuccess: test_c_api_null_label_clears passes consistently. Null label correctly clears device label. Other device_registry tests still pass. Fix is minimal and clear.\n\nAfter completing:\n1. Mark [-], implement fix, test locally, use log-implementation with detailed artifacts, mark [x]_\n\n- [ ] 2.2 Fix scripting documentation test\n  - File: core/src/scripting/docs/test_example.rs (test or macro/registry)\n  - Apply fix based on root cause from 1.3\n  - Add initialization or fix registration as needed\n  - Ensure macro generates documentation correctly\n  - Purpose: Make test pass\n  - _Leverage: Root cause analysis from 1.3_\n  - _Requirements: 2.1, 2.2, 2.3, 2.4, 2.5_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Rust developer with macro expertise\n\nTask: Fix doc test following requirements 2.1-2.5. Based on root cause from 1.3, apply appropriate fix: (A) If not initialized - add registry initialization in test setup, (B) If macro not working - fix macro or test annotation, (C) If wrong name - update test to use correct function name, (D) If isolation - set up proper test environment. Make minimal changes. Add comment explaining fix.\n\nRestrictions: Minimal fix. Don't refactor doc system. Preserve test intent. Ensure macro registration works. Don't break other doc tests.\n\nSuccess: test_macro_generates_doc passes consistently. Documentation registered correctly. Other doc tests still pass. Fix is minimal and clear.\n\nAfter completing:\n1. Mark [-], implement fix, test locally, use log-implementation with artifacts, mark [x]_\n\n## Phase 3: Verification and Testing\n\n- [ ] 3.1 Run specific fixed tests multiple times\n  - Command: `for i in {1..10}; do cargo test test_c_api_null_label_clears test_macro_generates_doc -- --exact || break; done`\n  - Verify both tests pass consistently\n  - Check for flakiness\n  - Purpose: Ensure fixes are stable\n  - _Leverage: Fixed tests from Phase 2_\n  - _Requirements: 3.1, 3.2, 3.3, 3.4, 3.5_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: QA stability tester\n\nTask: Verify test stability following requirement 3. Run both fixed tests 10 times in loop: `for i in {1..10}; do cargo test test_c_api_null_label_clears test_macro_generates_doc -- --exact || break; done`. Document if all 10 passes succeed or if any fail. Check for any non-deterministic behavior. If flaky, investigate and fix.\n\nRestrictions: Verification only. If tests are flaky, document but handle in separate fix task.\n\nSuccess: Both tests pass all 10 times. No flakiness. Tests are stable and reliable.\n\nAfter completing:\n1. Mark [-], run stability tests, document results, log findings, mark [x]_\n\n- [ ] 3.2 Run full library test suite\n  - Command: `cargo test --lib`\n  - Verify all library tests pass (2,440+ tests)\n  - Check for any new failures\n  - Purpose: Ensure no regressions in library\n  - _Leverage: cargo test_\n  - _Requirements: 3.1, 3.2, 3.3_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: QA regression tester\n\nTask: Run full library test suite following requirement 3. Execute `cargo test --lib --verbose` and capture output. Verify: (1) All tests pass, (2) Test count matches expected (~2,440), (3) No new failures introduced, (4) Fixed tests show as passing. Document any failures. Save output to .spec-workflow/specs/fix-failing-tests/test_results_lib.txt.\n\nRestrictions: Verification only. If other tests fail, document for follow-up but don't fix in this task.\n\nSuccess: All library tests pass. Test count correct. No regressions. Fixed tests confirmed passing. Results documented.\n\nAfter completing:\n1. Mark [-], run tests, capture output, log results, mark [x]_\n\n- [ ] 3.3 Run complete test suite\n  - Command: `cargo test --all`\n  - Verify all tests pass including integration and E2E tests\n  - Check total test count\n  - Purpose: Ensure full project test coverage\n  - _Leverage: cargo test_\n  - _Requirements: 3.1, 3.2, 3.3_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: QA comprehensive tester\n\nTask: Run complete test suite following requirement 3. Execute `cargo test --all --verbose` and capture output. Verify all tests pass across all packages. Document: (1) Total test count, (2) Pass/fail summary, (3) Any warnings or issues, (4) Test runtime. Save to .spec-workflow/specs/fix-failing-tests/test_results_all.txt.\n\nRestrictions: Verification only. Document any issues but don't fix.\n\nSuccess: All tests pass. Complete coverage verified. No failures anywhere. Results documented.\n\nAfter completing:\n1. Mark [-], run tests, document, log, mark [x]_\n\n- [ ] 3.4 Run clippy and verify no new warnings\n  - Command: `cargo clippy --all-targets -- -D warnings`\n  - Verify fixes didn't introduce warnings\n  - Check code quality maintained\n  - Purpose: Ensure code quality standards\n  - _Leverage: cargo clippy_\n  - _Requirements: 3.3_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Code quality specialist\n\nTask: Run clippy validation. Execute `cargo clippy --all-targets -- -D warnings`. Verify no warnings. If warnings exist from fixes, document them. Check specifically around modified code from fixes.\n\nRestrictions: Verification only. Document warnings but don't fix.\n\nSuccess: Clippy passes with no warnings. Code quality maintained. Any warnings documented.\n\nAfter completing:\n1. Mark [-], run clippy, document, log, mark [x]_\n\n## Phase 4: Enable and Measure Coverage\n\n- [ ] 4.1 Run code coverage measurement\n  - Command: `cargo llvm-cov --lib --summary-only`\n  - Measure overall code coverage percentage\n  - Generate detailed coverage report\n  - Purpose: Enable coverage metrics now that tests pass\n  - _Leverage: cargo llvm-cov_\n  - _Requirements: 4.1, 4.2, 4.3, 4.4_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Code coverage analyst\n\nTask: Measure code coverage following requirement 4. Run `cargo llvm-cov --lib --summary-only` to get overall percentage. Then run `cargo llvm-cov --lib --html` to generate detailed report. Document: (1) Overall coverage percentage, (2) Compare to 80% target, (3) Critical path modules coverage, (4) Compare to 90% target for critical paths. Save summary to .spec-workflow/specs/fix-failing-tests/coverage_results.txt.\n\nRestrictions: Measurement only. Don't add tests to improve coverage yet.\n\nSuccess: Coverage successfully measured. Overall percentage known. Critical paths identified. Gaps documented. Report generated.\n\nAfter completing:\n1. Mark [-], measure coverage, document results, log findings, mark [x]_\n\n- [ ] 4.2 Document coverage gaps if any\n  - File: .spec-workflow/specs/fix-failing-tests/coverage_gaps.md\n  - Identify modules below 80% coverage\n  - Identify critical paths below 90% coverage\n  - Purpose: Plan future coverage improvements\n  - _Leverage: Coverage report from 4.1_\n  - _Requirements: 4.2, 4.3_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Code coverage analyst\n\nTask: Document coverage gaps following requirements 4.2-4.3. Review HTML coverage report from 4.1. Identify: (1) Modules below 80% overall, (2) Critical paths (services, api, engine, ffi) below 90%, (3) Specific files/functions with low coverage, (4) Recommend priorities for adding tests. Create table with module, current coverage, target, gap. Document in coverage_gaps.md.\n\nRestrictions: Documentation only. Don't write tests yet. Provide actionable recommendations.\n\nSuccess: All coverage gaps documented with specific line numbers. Priorities identified. Recommendations clear. Serves as roadmap for future test additions.\n\nAfter completing:\n1. Mark [-], analyze and document gaps, log findings, mark [x]_\n\n## Phase 5: CI Validation and Documentation\n\n- [ ] 5.1 Run full CI checks\n  - Command: `just ci-check` or manual CI steps\n  - Verify all CI steps pass: fmt, clippy, test, doc\n  - Ensure CI pipeline is green\n  - Purpose: Final validation before completion\n  - _Leverage: Project CI configuration_\n  - _Requirements: 3.1, 3.5_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: CI/CD specialist\n\nTask: Run full CI validation following requirement 3. Execute `just ci-check` or run manually: (1) cargo fmt --check, (2) cargo clippy --all-targets -- -D warnings, (3) cargo test --all, (4) cargo doc --no-deps. Verify all pass. Document results. Capture any failures.\n\nRestrictions: Validation only. If CI fails, document for follow-up.\n\nSuccess: All CI checks pass. Pipeline green. Project ready for merge. Results documented.\n\nAfter completing:\n1. Mark [-], run CI, document, log results, mark [x]_\n\n- [ ] 5.2 Update CODEBASE_EVALUATION.md with results\n  - File: CODEBASE_EVALUATION.md\n  - Add section documenting test fix results\n  - Record coverage measurements\n  - Note impact on CI/CD\n  - Purpose: Document completion of #3 priority\n  - _Leverage: All verification results_\n  - _Requirements: All_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Technical writer\n\nTask: Update evaluation document. Add section \"# Implementation Results - Fix Failing Tests\" at end of CODEBASE_EVALUATION.md. Document: (1) Tests fixed - list both tests, (2) Root causes - explain what was wrong, (3) Fixes applied - what changed, (4) Test results - 2,440/2,440 passing, (5) Coverage enabled - percentage achieved, (6) CI status - now passing, (7) Time taken - actual vs estimated. Use clear format with before/after comparison.\n\nRestrictions: Be factual and specific. Use actual measurements. Document lessons learned.\n\nSuccess: Evaluation updated with comprehensive results. Impact clear. Stakeholders see value. Future reference available.\n\nAfter completing:\n1. Mark [-], update docs, log update, mark [x]_\n\n- [ ] 5.3 Document fix details for future reference\n  - File: .spec-workflow/specs/fix-failing-tests/FIX_SUMMARY.md\n  - Create detailed fix documentation\n  - Include root causes, solutions, prevention tips\n  - Purpose: Help future developers understand fixes\n  - _Leverage: All analysis and fix work_\n  - _Requirements: All_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Technical documentation specialist\n\nTask: Create fix summary document. Structure: (1) Executive Summary - both tests fixed, (2) Test 1 Details - what failed, why, how fixed, (3) Test 2 Details - what failed, why, how fixed, (4) Prevention - how to avoid similar issues, (5) Lessons Learned - key takeaways. Include code snippets showing before/after. Keep under 300 lines.\n\nRestrictions: Write for future developers. Be clear and educational. Include concrete examples.\n\nSuccess: Fix summary created. Comprehensive reference for similar issues. Helps prevent recurrence. Educational for team.\n\nAfter completing:\n1. Mark [-], write summary, log creation, mark [x]_\n\n## Phase 6: Cleanup and Final Steps\n\n- [ ] 6.1 Remove temporary debug code if any\n  - Files: Any files with debug output added during investigation\n  - Clean up println!, eprintln!, dbg! macros added for debugging\n  - Ensure no debug artifacts remain\n  - Purpose: Clean codebase\n  - _Leverage: Investigation work from Phase 1_\n  - _Requirements: Maintainability_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Code cleanup specialist\n\nTask: Remove debug code. Search for: (1) println! or eprintln! added during investigation, (2) dbg! macros, (3) TODO comments added for tracking, (4) Temporary files created during debugging. Remove all debugging artifacts. Ensure code is clean.\n\nRestrictions: Only remove temporary debug code. Don't remove intentional logging or test output.\n\nSuccess: No debug artifacts remain. Code is clean. Codebase ready for commit.\n\nAfter completing:\n1. Mark [-], clean code, verify clean, log cleanup, mark [x]_\n\n- [ ] 6.2 Format code and verify consistency\n  - Command: `cargo fmt`\n  - Ensure all modified code properly formatted\n  - Verify formatting check passes\n  - Purpose: Maintain code formatting standards\n  - _Leverage: cargo fmt_\n  - _Requirements: Code quality_\n  - _Prompt: Implement the task for spec fix-failing-tests. First run spec-workflow-guide to get the workflow guide, then implement the task:\n\nRole: Code formatting specialist\n\nTask: Format code. Run `cargo fmt` to auto-format all modified files. Then run `cargo fmt --check` to verify. Ensure formatting is consistent with project standards.\n\nRestrictions: Just run formatter. Trust cargo fmt output.\n\nSuccess: All code formatted correctly. `cargo fmt --check` passes. Code style consistent.\n\nAfter completing:\n1. Mark [-], format, verify, log, mark [x]_\n\n## Summary\n\n**Total Tasks:** 16 tasks across 6 phases\n\n**Estimated Effort:** 1-2 hours\n\n**Expected Impact:**\n- ✅ CI/CD pipeline unblocked (was broken)\n- ✅ All 2,440 tests passing (was 2,438/2,440)\n- ✅ Code coverage measurable (was blocked)\n- ✅ Quality metrics enabled\n- ✅ Confident PR merges\n- ✅ Fast turnaround (<2 hours actual vs 1-2 hour estimate)\n\n**Key Deliverables:**\n- 2 tests fixed with documented root causes\n- Full test suite passing (100% pass rate)\n- Code coverage measured and documented\n- CI pipeline green\n- Comprehensive fix documentation for future reference\n- Prevention guidelines to avoid similar issues\n",
  "fileStats": {
    "size": 19607,
    "lines": 364,
    "lastModified": "2025-12-11T16:00:32.968Z"
  },
  "comments": []
}