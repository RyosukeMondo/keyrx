{
  "id": "snapshot_1765468895046_o0cip5vn7",
  "approvalId": "approval_1765468849317_krwagchef",
  "approvalTitle": "Fix Failing Tests - Design Document",
  "version": 2,
  "timestamp": "2025-12-11T16:01:35.046Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Design Document\n\n## Overview\n\nThis design fixes 2 failing tests blocking CI/CD pipeline. Each fix follows root cause analysis → minimal fix → verification pattern.\n\n**Core Principles:**\n1. **Understand root cause** before fixing\n2. **Minimal changes** - only fix what's broken\n3. **Verify fix** - ensure test passes reliably\n4. **No regressions** - other tests still pass\n\n## Test Failure Analysis\n\n### Failure 1: test_c_api_null_label_clears\n\n**Location:** `core/src/ffi/domains/device_registry.rs:571`\n\n**Error:**\n```\nassertion failed: msg.starts_with(\"ok:\")\n```\n\n**Test Purpose:** Validate that passing null label to FFI clears device label\n\n**Code Context:**\n```rust\n#[test]\nfn test_c_api_null_label_clears() {\n    // Test expects response starting with \"ok:\"\n    let result = set_device_label_c_api(device_key, std::ptr::null());\n    let msg = unsafe { CStr::from_ptr(result).to_str().unwrap() };\n    assert!(msg.starts_with(\"ok:\"));  // ❌ FAILING HERE\n}\n```\n\n**Possible Root Causes:**\n1. **Response format changed** - Implementation now returns different format\n2. **Error handling changed** - Null now treated as error instead of clear\n3. **Serialization changed** - JSON format modified\n4. **Test expectation wrong** - Test expects incorrect format\n\n**Investigation Steps:**\n1. Read current `set_device_label` implementation\n2. Check what response format is actually returned\n3. Verify null handling logic\n4. Check if error response returned instead of success\n\n**Fix Strategies:**\n- **If format changed:** Update test expectation to match new format\n- **If null handling changed:** Update implementation to handle null as clear\n- **If error returned:** Fix implementation to treat null as clear, not error\n- **If test wrong:** Fix test to expect correct format\n\n---\n\n### Failure 2: test_macro_generates_doc\n\n**Location:** `core/src/scripting/docs/test_example.rs:46`\n\n**Error:**\n```\nDocumentation should be registered\n```\n\n**Test Purpose:** Validate that `#[rhai_doc]` macro registers documentation\n\n**Code Context:**\n```rust\n#[test]\nfn test_macro_generates_doc() {\n    // Test expects doc to be registered\n    let doc = get_function_doc(\"test_function\");\n    assert!(doc.is_some(), \"Documentation should be registered\");  // ❌ FAILING HERE\n}\n```\n\n**Possible Root Causes:**\n1. **Registry not initialized** - Doc registry needs setup in test\n2. **Macro not expanded** - Macro didn't run or failed\n3. **Registration logic changed** - Registration happens differently now\n4. **Test isolation issue** - Test doesn't initialize required state\n\n**Investigation Steps:**\n1. Read doc registry initialization code\n2. Check if `initialize()` needs to be called\n3. Verify macro expansion in compiled code\n4. Check other passing doc tests for setup pattern\n\n**Fix Strategies:**\n- **If not initialized:** Call `initialize_doc_registry()` in test setup\n- **If macro issue:** Fix macro or test annotation\n- **If registration changed:** Update test to use new registration method\n- **If isolation:** Set up proper test environment\n\n## Implementation Plan\n\n### Fix 1: FFI Device Registry Test\n\n**Step 1: Investigate Current Behavior**\n```rust\n// Add debug output to understand actual response\n#[test]\nfn test_c_api_null_label_clears() {\n    let result = set_device_label_c_api(device_key, std::ptr::null());\n    let msg = unsafe { CStr::from_ptr(result).to_str().unwrap() };\n    eprintln!(\"Actual response: {:?}\", msg);  // See what it returns\n    assert!(msg.starts_with(\"ok:\"));\n}\n```\n\n**Step 2: Apply Fix Based on Investigation**\n\n**Scenario A: Format changed (most likely)**\n```rust\n// Update test expectation\nassert!(msg.starts_with(\"success:\"));  // Or whatever new format is\n// OR\nassert!(msg.contains(\"\\\"status\\\":\\\"ok\\\"\"));  // If JSON format changed\n```\n\n**Scenario B: Null handling broken**\n```rust\n// Fix implementation to handle null correctly\npub fn set_device_label(device_key: &str, label: Option<String>) -> Result<String> {\n    // Ensure None/null clears label\n    if label.is_none() {\n        return clear_label(device_key);  // Explicit clear\n    }\n    // ... rest of logic\n}\n```\n\n**Scenario C: Error returned**\n```rust\n// Fix error handling\nif label.is_null() {\n    // Don't return error, return success with clear operation\n    return success_response(\"Label cleared\");\n}\n```\n\n---\n\n### Fix 2: Scripting Documentation Test\n\n**Step 1: Investigate Registry Initialization**\n```rust\n// Check if registry needs initialization\n#[test]\nfn test_macro_generates_doc() {\n    // Try initializing registry\n    crate::scripting::docs::registry::initialize();\n\n    let doc = get_function_doc(\"test_function\");\n    eprintln!(\"Doc found: {:?}\", doc);  // Debug output\n    assert!(doc.is_some(), \"Documentation should be registered\");\n}\n```\n\n**Step 2: Apply Fix Based on Investigation**\n\n**Scenario A: Missing initialization (most likely)**\n```rust\n#[test]\nfn test_macro_generates_doc() {\n    // Initialize doc registry before test\n    crate::scripting::docs::registry::initialize();\n\n    let doc = get_function_doc(\"test_function\");\n    assert!(doc.is_some(), \"Documentation should be registered\");\n}\n```\n\n**Scenario B: Macro not registering**\n```rust\n// Fix test to explicitly register doc\n#[test]\nfn test_macro_generates_doc() {\n    // Manually trigger registration if macro didn't\n    register_test_function_doc();\n\n    let doc = get_function_doc(\"test_function\");\n    assert!(doc.is_some());\n}\n```\n\n**Scenario C: Wrong function name**\n```rust\n#[test]\nfn test_macro_generates_doc() {\n    // Check actual registered name\n    let doc = get_function_doc(\"test_function_impl\");  // Might have suffix\n    assert!(doc.is_some());\n}\n```\n\n## Testing Strategy\n\n### Per-Fix Verification\n\n**After each fix:**\n\n1. **Run specific test:**\n```bash\ncargo test test_c_api_null_label_clears -- --exact\ncargo test test_macro_generates_doc -- --exact\n```\n\n2. **Run related tests:**\n```bash\ncargo test device_registry::tests  # For fix 1\ncargo test scripting::docs::tests  # For fix 2\n```\n\n3. **Run full suite:**\n```bash\ncargo test --lib\ncargo test --all\n```\n\n### Integration Verification\n\n**After both fixes:**\n\n1. **CI checks:**\n```bash\njust ci-check\n# OR manually:\ncargo fmt --check\ncargo clippy --all-targets -- -D warnings\ncargo test --all\ncargo doc --no-deps\n```\n\n2. **Coverage measurement:**\n```bash\ncargo llvm-cov --lib --summary-only\ncargo llvm-cov --lib --html  # Generate detailed report\n```\n\n3. **Stability check:**\n```bash\n# Run tests multiple times to check for flakiness\nfor i in {1..10}; do cargo test --lib || break; done\n```\n\n## Error Handling\n\n### If Fix Doesn't Work\n\n**Test still fails after first attempt:**\n\n1. **Add more debug output:**\n```rust\neprintln!(\"Full context: {:?}\", debug_context);\n```\n\n2. **Check git history:**\n```bash\ngit log --oneline -20 -- path/to/test/file.rs\ngit show <commit> -- path/to/test/file.rs\n```\n\n3. **Compare with similar passing tests:**\n```rust\n// Find similar test that passes\n// Copy its setup/pattern\n```\n\n4. **Consult documentation:**\n```rust\n// Read module docs\ncargo doc --open --package keyrx-core\n```\n\n### If Fix Breaks Other Tests\n\n**Regressions introduced:**\n\n1. **Identify broken tests:**\n```bash\ncargo test --lib 2>&1 | grep FAILED\n```\n\n2. **Analyze impact:**\n```rust\n// Check what functionality changed\n// Verify if intentional or bug\n```\n\n3. **Refine fix:**\n```rust\n// Make fix more targeted\n// Add conditions if needed\n```\n\n## Documentation\n\n### Document Root Cause\n\n**Add comment in test explaining the issue:**\n```rust\n#[test]\nfn test_c_api_null_label_clears() {\n    // Note: Response format changed from \"ok:\" to \"success:\" in commit abc123\n    // This test validates null label clears device label via FFI\n    let result = set_device_label_c_api(device_key, std::ptr::null());\n    // ...\n}\n```\n\n### Document Fix in Commit Message\n\n**Commit message format:**\n```\nfix(tests): Fix failing device registry FFI test\n\nThe test was expecting \"ok:\" response format but implementation\nchanged to return \"success:\" format in commit abc123.\n\nUpdated test expectation to match current implementation.\n\nTest: test_c_api_null_label_clears\nFile: core/src/ffi/domains/device_registry.rs:571\n```\n\n### Update Test Documentation\n\n**Add doc comment if missing:**\n```rust\n/// Test that passing null label to FFI API correctly clears device label.\n///\n/// This validates the FFI boundary correctly handles null pointers\n/// and translates them to Option::None, which should clear the label.\n#[test]\nfn test_c_api_null_label_clears() {\n    // ...\n}\n```\n\n## Backward Compatibility\n\n### No Breaking Changes Expected\n\n- Tests are internal (not public API)\n- Fixes should not change any public interfaces\n- Implementation changes should be minimal\n- FFI contract should remain stable\n\n### If Breaking Change Needed\n\n**If fix requires API change:**\n\n1. **Document in CHANGELOG**\n2. **Update FFI contracts if affected**\n3. **Update Dart bindings if needed**\n4. **Add migration notes**\n\n**Unlikely for test fixes** - typically just test expectations need updating\n\n## Performance Considerations\n\n### Test Runtime\n\n- Fixed tests should complete in <100ms\n- No performance impact expected\n- Test suite total time should remain <3s\n\n### CI Impact\n\n**Before fixes:**\n- CI fails immediately on test failure\n- Cannot proceed to coverage/deploy\n\n**After fixes:**\n- CI completes successfully\n- Coverage measurement enabled\n- ~2 minutes total CI time (unchanged)\n\n## Migration Plan\n\n### Phase 1: Investigation (15 minutes)\n\n1. Run tests to reproduce failures\n2. Add debug output to understand root cause\n3. Check git history for relevant changes\n4. Identify fix strategy\n\n### Phase 2: Implementation (30 minutes)\n\n1. Apply fix for test 1\n2. Verify fix works\n3. Apply fix for test 2\n4. Verify fix works\n\n### Phase 3: Verification (15 minutes)\n\n1. Run full test suite\n2. Run CI checks\n3. Measure coverage\n4. Document fixes\n\n**Total estimated time:** 1 hour\n",
  "fileStats": {
    "size": 9879,
    "lines": 405,
    "lastModified": "2025-12-11T16:00:32.883Z"
  },
  "comments": []
}